<!DOCTYPE HTML>
<!--
	Solid State by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Generic - Solid State by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Expansion in an Artificial Neural Network</a></h1>
					</header>

				<!-- Wrapper -->
					<section id="wrapper">
						<header>
							<div class="inner">
								<h2>Computational Benefits of Expansion and Sparsity in Neural Networks</h2>
								<p>We explore various possible ways an artifical neural network benefits from expansion and sparsity in it's hidden layers.</p>
							</div>
						</header>

						<!-- Content -->
							<div class="wrapper">
								<div class="inner">

									<h3 class="major">Overview</h3>
									<p>Stimuli from our external world sent into numerous sensory pathways are projected to downstream populations which are sparsely active with a higher density of neurons than arriving axons. This project seeks to explore the computational advantages of expansion and sparsity for inputs that are clustered, where various clusters correspond to behaviorally diverse stimuli and intracluster variability to sensory or neural noise. This project is inspired by a neuron article Babadi and Sompolinsky published in 2014. Through mathematical simulations and analytical calculations, they demonstrate that the feed-forward random synaptic weights used for expansion amplify the variability of incoming inputs, and that the noise amplification gets worse as the expanded representation becomes more sparse. The task which their learning algorithm was performed on was binary classification but I focus on a linear regression task to explore more ways in which expansion and sparsity can be beneficial computationally.</p>

									<h3 class="major">Result</h3>
									<p>The snippet below shows the graph obtained from the simulation.</p>

									<section class="features">
										<article>
											<a class="image"><img src="images/per.jpg" alt="" /></a>
											<h3 class="major">Perceptron</h3>
											<p>The figure above shows the shematic of a perceptron showing j weight which are fixed and random, Ns indicating number of units in input layer drawn from a random normal gaussian distribution, Nc showing number of units in hidden layer, W showing learned weights and y showing output. </p>
										</article>
										<article>
											<a class="image"><img src="images/graph.png" alt="" /></a>
											<h3 class="major">Expansion of Hidden Size of a Perceptron</h3>
											<p>The figure above shows how error decreases as the units of the hidden size of a perceptron expands.</p>
										</article>
									</section>

								</div>
							</div>

					</section>

				<!-- Footer -->
					<section id="footer">
						<div class="inner">
							<h2 class="major">More Information</h2>
							<p>Click the github link below to get more information on project.</p>
						
							<ul class="contact">
								<li class="icon brands fa-github"><a href="https://github.com/NathanielAdibuer/ExpansionInPerceptron">github.com/NathanielAdibuer</a></li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>